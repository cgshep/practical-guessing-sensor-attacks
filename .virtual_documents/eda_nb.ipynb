import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy as sp
import numpy as np


permitted_cols = ['seconds_elapsed', 'accelerometer_z', 'accelerometer_y',
       'accelerometer_x', 'gravity_z', 'gravity_y', 'gravity_x', 'gyroscope_z',
       'gyroscope_y', 'gyroscope_x', 'orientation_qz', 'orientation_qy',
       'orientation_qx', 'orientation_qw', 'orientation_roll',
       'orientation_pitch', 'orientation_yaw', 'magnetometer_z',
       'magnetometer_y', 'magnetometer_x', 'compass_magneticBearing',
       'barometer_relativeAltitude', 'barometer_pressure',
       'location_bearing', 'location_altitude', 'location_longitude', 
       'location_latitude', 'microphone_dBFS', 'light_lux']

pixel_garden = pd.read_csv("data/garden/GardenPixel-2025-03-23_17-03-12-8132b4b89fb54aa7a9762a4a82d6863d.csv",
                          usecols=permitted_cols).dropna()


display(pixel_garden.columns)
display(pixel_garden.shape)





sns.set_theme(style="whitegrid")
cmap = sns.diverging_palette(230, 20, as_cmap=True)
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(pixel_garden.drop("seconds_elapsed", axis=1).corr(), cmap=cmap)


def shannon_entropy(series, base=2):    
    # Calculate the frequency of each unique value.
    counts = series.value_counts()
    
    # Convert counts to probabilities.
    probabilities = counts / counts.sum()
    
    # Calculate the entropy using the formula:
    # H(X) = - sum(p_i * log(p_i))
    # The logarithm base can be adjusted; here, we use natural log and convert by dividing by log(base).
    return -np.sum(probabilities * np.log(probabilities)) / np.log(base)


def expected_guessing_entropy(series):    
    # Compute the frequency of each unique value.
    # .value_counts() returns counts sorted in descending order by default.
    counts = series.value_counts()
    
    # Convert counts to probabilities.
    probabilities = counts / counts.sum()
    
    # Ensure probabilities are sorted in descending order.
    probabilities = probabilities.sort_values(ascending=False)
    
    # Create an array of ranks starting from 1 to the number of unique outcomes.
    ranks = np.arange(1, len(probabilities) + 1)
    
    # Return expected guessing entropy as the weighted sum of ranks.
    return np.sum(ranks * probabilities.values)

def marginal_guesswork(series, alpha=0.9):
    # Calculate frequency counts (sorted descending by default).
    counts = series.value_counts()
    
    # Convert counts to probabilities.
    probabilities = counts / counts.sum()
    
    # Compute cumulative probabilities.
    cum_probs = probabilities.cumsum().values
    
    # When alpha is 1 (or nearly 1), return the total number of unique outcomes.
    if np.isclose(alpha, 1.0):
        return len(cum_probs)
    
    # Find the smallest index k where the cumulative probability >= alpha.
    # np.argmax returns the first occurrence where the condition is True.
    k = np.argmax(cum_probs >= alpha) + 1  # +1 because guesses are 1-indexed.
    return k

def massey_bound(series):
    return 0.25 * ((2**shannon_entropy(series))-1)


def display_entropies(series):
    display(f"H(X)    = {shannon_entropy(series)}")
    display(f"E[G(X)] â‰¥ {massey_bound(series)}")
    display(f"E[G(X)] = {expected_guessing_entropy(series)}")


display_entropies(pixel_garden["accelerometer_x"])


def simulate_average_guesses(series, n=1000):    
    # Compute frequency counts; value_counts() returns counts sorted in descending order.
    counts = series.value_counts()
    
    # The guess order is fixed:
    guess_order = counts.index.to_numpy()  # Unique outcomes in sorted order (most frequent first)
    
    # Compute the probabilities corresponding to the guess order.
    probabilities = counts.to_numpy() / counts.to_numpy().sum()
    
    # Instead of looping, sample indices (representing the rank) directly in a vectorized manner.
    # Since guess_order is in descending order, if an outcome is at index i, the number of guesses is i+1.
    sampled_indices = np.random.choice(len(guess_order), size=n, p=probabilities)
    # Compute the average number of guesses (adding 1 because ranks are 0-indexed).
    avg_guesses = np.mean(sampled_indices + 1)
    return avg_guesses


marginal_guesswork(pixel_garden["accelerometer_x"])


simulate_average_guesses(pixel_garden["accelerometer_x"])



